{
  "model_name": "llama3",
  "provider": "ollama",
  "temperature": 0.2,
  "max_tokens": 1024
}
